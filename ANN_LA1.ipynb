{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**BINARY CLASSIFIFCATION WITH PERCEPTRON**"
      ],
      "metadata": {
        "id": "xiPXXkACUQV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training the perceptron\n",
        "        for _ in range(self.n_iterations):\n",
        "            for i in range(n_samples):\n",
        "                # Compute the predicted output\n",
        "                predicted = np.dot(X[i], self.weights) + self.bias\n",
        "                # Apply step function as activation\n",
        "                predicted = 1 if predicted >= 0 else 0\n",
        "                # Update weights and bias based on error\n",
        "                self.weights += self.learning_rate * (y[i] - predicted) * X[i]\n",
        "                self.bias += self.learning_rate * (y[i] - predicted)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute predicted output for each sample\n",
        "        predicted = np.dot(X, self.weights) + self.bias\n",
        "        # Apply step function as activation\n",
        "        predicted = np.where(predicted >= 0, 1, 0)\n",
        "        return predicted\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate synthetic dataset\n",
        "    X = np.array([[2, 3], [1, 4], [2, 5], [6, 1], [7, 2], [5, 3]])\n",
        "    y = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "    # Initialize and train the perceptron\n",
        "    perceptron = Perceptron(learning_rate=0.01, n_iterations=1000)\n",
        "    perceptron.fit(X, y)\n",
        "\n",
        "    # Predict on new data\n",
        "    test_samples = np.array([[3, 2], [4, 5], [6, 3]])\n",
        "    predictions = perceptron.predict(test_samples)\n",
        "    print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "id": "uRCob3RrPy2b",
        "outputId": "7f68cfb8-be73-4bfe-a546-6d3dab033f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Perceptron class with methods for training (fit) and making predictions (predict). The perceptron learns to classify samples into two classes (0 and 1) based on their features using the provided training data. The fit method updates the weights and bias iteratively using the training data, and the predict method predicts the class labels for new samples."
      ],
      "metadata": {
        "id": "cQUNxUu5Q7Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRADIENT DESCENT**"
      ],
      "metadata": {
        "id": "ZuwodW_JUG-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(gradient_func, initial_position, learning_rate=0.1, epsilon=1e-6, max_iterations=1000):\n",
        "    position = initial_position\n",
        "    for i in range(max_iterations):\n",
        "        gradient = gradient_func(position)\n",
        "        if np.linalg.norm(gradient) < epsilon:\n",
        "            break\n",
        "        position -= learning_rate * gradient\n",
        "    return position\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return 2 * x\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    initial_position = 10\n",
        "    minimum = gradient_descent(quadratic_function, initial_position)\n",
        "    print(\"Minimum found at:\", minimum)\n"
      ],
      "metadata": {
        "id": "InKKP8ApSet9",
        "outputId": "9b2ea634-0934-4706-d774-451adc21869c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum found at: 4.313591466744105e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. gradient_descent is the main function that performs the gradient descent optimization. It takes the following parameters:\n",
        "\n",
        "* gradient_func: A function that computes the gradient of the objective function at a given position.\n",
        "* initial_position: The initial position to start the optimization.\n",
        "* learning_rate: The step size or learning rate for updating the position.\n",
        "* epsilon: A small value representing the convergence criterion. If the norm of the gradient falls below this value, the optimization stops.\n",
        "* max_iterations: The maximum number of iterations to perform if convergence is not achieved.\n",
        "2. quadratic_function is a simple quadratic function\n",
        "f(x)=2x used as an example objective function.\n",
        "\n",
        "3. The code then initializes an initial position and calls the gradient_descent function to find the minimum of the quadratic function using gradient descent. Finally, it prints out the minimum found."
      ],
      "metadata": {
        "id": "1snHcVwUShOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return 2 * x\n",
        "\n",
        "def gradient(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        y = quadratic_function(x)\n",
        "    return tape.gradient(y, x)\n",
        "\n",
        "def gradient_descent(initial_position, learning_rate=0.1, epsilon=1e-6, max_iterations=1000):\n",
        "    position = tf.Variable(initial_position, dtype=tf.float32)\n",
        "    for i in range(max_iterations):\n",
        "        grad = gradient(position)\n",
        "        if tf.norm(grad) < epsilon:\n",
        "            break\n",
        "        position.assign_sub(learning_rate * grad)\n",
        "    return position.numpy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    initial_position = tf.constant(10.0)\n",
        "    minimum = gradient_descent(initial_position)\n",
        "    print(\"Minimum found at:\", minimum)\n"
      ],
      "metadata": {
        "id": "RW8aQOocQAAS",
        "outputId": "6652de88-274a-466c-818b-9198ee141779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum found at: -189.99825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. quadratic_function is the same as before, defining a simple quadratic function\n",
        "f(x)=2x.\n",
        "\n",
        "2. gradient is a function that uses TensorFlow's automatic differentiation capabilities (tf.GradientTape) to compute the gradient of the quadratic function with respect to the input variable x.\n",
        "\n",
        "3. gradient_descent is similar to the previous implementation but uses TensorFlow variables and operations. It iteratively updates the position using the gradients computed by the gradient function.\n",
        "\n",
        "4. The rest of the code remains the same as before, initializing the initial position and calling the gradient_descent function to find the minimum of the quadratic function using TensorFlow.\n",
        "\n",
        "5. Using TensorFlow allows you to leverage its computational graph and automatic differentiation capabilities, making it easier to perform gradient-based optimization tasks like gradient descent."
      ],
      "metadata": {
        "id": "cLr1Wk9-QSae"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}